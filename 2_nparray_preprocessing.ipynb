{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hyperparameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE': 90,\n",
    "    'PREDICT_SIZE': 21,\n",
    "    'EPOCHS': 10,\n",
    "    'SEED': 41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = pd.read_csv('preprocess_train_data.csv').drop(columns=['제품']).fillna(0)\n",
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = train1.groupby('ID')\n",
    "\n",
    "scale_min_dict = {}\n",
    "scale_max_dict = {}\n",
    "\n",
    "for name, group in groups:\n",
    "    scale_min_dict[name] = group['판매량'].min()\n",
    "    scale_max_dict[name] = group['판매량'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "def scale_series(s):\n",
    "    return pd.Series(scaler.fit_transform(s.values.reshape(-1, 1)).flatten(), index=s.index)\n",
    "\n",
    "train1['판매량'] = train1.groupby('ID')['판매량'].transform(scale_series)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train1['개당판매금액'] = scaler.fit_transform(train1['개당판매금액'].values.reshape(-1,1))\n",
    "train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train1['판매량']\n",
    "train1.drop(['판매량'], axis=1, inplace=True)\n",
    "train1['판매량'] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train1[col])\n",
    "    train1[col] = label_encoder.transform(train1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(group, train_size, predict_size):\n",
    "    group = group.reset_index(drop=True)\n",
    "    window_size = train_size + predict_size\n",
    "\n",
    "    sale_data = group.iloc[:,-1:]\n",
    "    means = sale_data.rolling(window=train_size).mean().values.flatten()\n",
    "    stds = sale_data.rolling(window=train_size).std().values.flatten()\n",
    "    \n",
    "    group.drop(['판매량'], axis=1, inplace=True)\n",
    "\n",
    "    input_data, target_data = [], []\n",
    "    for j in range(len(group) - window_size):\n",
    "        group.loc[j:j+train_size, 'mean'] = means[j+train_size]\n",
    "        group.loc[j:j+train_size, 'std'] = stds[j+train_size]\n",
    "        group.loc[j:j+train_size, '판매량'] = sale_data[j:j+train_size]\n",
    "        input_data.append(group.iloc[j:j+train_size].values)\n",
    "        target_data.append(sale_data.iloc[j+train_size:j+window_size].values)\n",
    "    return input_data, target_data\n",
    "\n",
    "def make_train_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    grouped = data.groupby('ID')\n",
    "    input_data, target_data = [], []\n",
    "    \n",
    "    results = Parallel(n_jobs=-1)(delayed(process_group)(group, train_size, predict_size) for _, group in tqdm(grouped, desc='Processing Groups'))\n",
    "\n",
    "    for result in results:\n",
    "        input_data.extend(result[0])\n",
    "        target_data.extend(result[1])\n",
    "    return np.array(input_data), np.squeeze(target_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_input, train1_target = make_train_data(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_test_data(data, train_size=CFG['TRAIN_WINDOW_SIZE'], predict_size=CFG['PREDICT_SIZE']):\n",
    "    test_data = []\n",
    "    data_group = data.groupby('ID')\n",
    "\n",
    "    for _, group in tqdm(data_group):\n",
    "        sale_data = group.iloc[:,-1:]\n",
    "\n",
    "        means = sale_data.rolling(window=train_size).mean().values.flatten()\n",
    "        stds = sale_data.rolling(window=train_size).std().values.flatten()\n",
    "\n",
    "        group.loc[-train_size:, 'mean'] = means[-predict_size]\n",
    "        group.loc[-train_size:, 'std'] = stds[-predict_size]\n",
    "\n",
    "        target_y = group['판매량']\n",
    "        group.drop(['판매량'], axis=1, inplace=True)\n",
    "        group['판매량'] = target_y\n",
    "        \n",
    "        test_data.append(group.tail(train_size).values)\n",
    "    return np.array(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = make_test_data(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_input.shape, train1_target.shape, test_input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## npy save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data/new_data/train1_input_mean_stds.npy', train1_input)\n",
    "np.save('./data/new_data/train1_target_mean_stds.npy', train1_target)\n",
    "np.save('./data/new_data/test_input_mean_stds.npy', test_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
