{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'TRAIN_WINDOW_SIZE':90, # 90일치로 학습\n",
    "    'PREDICT_SIZE':21, # 21일치 예측\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':3e-4,\n",
    "    'BATCH_SIZE':2048,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed_all(seed) # multi-gpu seed 고정\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>대분류</th>\n",
       "      <th>중분류</th>\n",
       "      <th>소분류</th>\n",
       "      <th>브랜드</th>\n",
       "      <th>개당판매금액</th>\n",
       "      <th>언급량</th>\n",
       "      <th>판매량</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0007</td>\n",
       "      <td>B002-C003-0038</td>\n",
       "      <td>B002-00001</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>0.84131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0007</td>\n",
       "      <td>B002-C003-0038</td>\n",
       "      <td>B002-00001</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>0.91383</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0007</td>\n",
       "      <td>B002-C003-0038</td>\n",
       "      <td>B002-00001</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>1.45053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0007</td>\n",
       "      <td>B002-C003-0038</td>\n",
       "      <td>B002-00001</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>2.42239</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0007</td>\n",
       "      <td>B002-C003-0038</td>\n",
       "      <td>B002-00001</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>1.87119</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293505</th>\n",
       "      <td>15889</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0004</td>\n",
       "      <td>B002-C003-0020</td>\n",
       "      <td>B002-03799</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>5.51203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293506</th>\n",
       "      <td>15889</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0004</td>\n",
       "      <td>B002-C003-0020</td>\n",
       "      <td>B002-03799</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>3.52480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293507</th>\n",
       "      <td>15889</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0004</td>\n",
       "      <td>B002-C003-0020</td>\n",
       "      <td>B002-03799</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>4.03249</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293508</th>\n",
       "      <td>15889</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0004</td>\n",
       "      <td>B002-C003-0020</td>\n",
       "      <td>B002-03799</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>5.88917</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7293509</th>\n",
       "      <td>15889</td>\n",
       "      <td>B002-C001-0002</td>\n",
       "      <td>B002-C002-0004</td>\n",
       "      <td>B002-C003-0020</td>\n",
       "      <td>B002-03799</td>\n",
       "      <td>49800.0</td>\n",
       "      <td>5.07687</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7293510 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID             대분류             중분류             소분류         브랜드  \\\n",
       "0            0  B002-C001-0002  B002-C002-0007  B002-C003-0038  B002-00001   \n",
       "1            0  B002-C001-0002  B002-C002-0007  B002-C003-0038  B002-00001   \n",
       "2            0  B002-C001-0002  B002-C002-0007  B002-C003-0038  B002-00001   \n",
       "3            0  B002-C001-0002  B002-C002-0007  B002-C003-0038  B002-00001   \n",
       "4            0  B002-C001-0002  B002-C002-0007  B002-C003-0038  B002-00001   \n",
       "...        ...             ...             ...             ...         ...   \n",
       "7293505  15889  B002-C001-0002  B002-C002-0004  B002-C003-0020  B002-03799   \n",
       "7293506  15889  B002-C001-0002  B002-C002-0004  B002-C003-0020  B002-03799   \n",
       "7293507  15889  B002-C001-0002  B002-C002-0004  B002-C003-0020  B002-03799   \n",
       "7293508  15889  B002-C001-0002  B002-C002-0004  B002-C003-0020  B002-03799   \n",
       "7293509  15889  B002-C001-0002  B002-C002-0004  B002-C003-0020  B002-03799   \n",
       "\n",
       "          개당판매금액      언급량  판매량  \n",
       "0        13500.0  0.84131    0  \n",
       "1        13500.0  0.91383    0  \n",
       "2        13500.0  1.45053    0  \n",
       "3        13500.0  2.42239    0  \n",
       "4        13500.0  1.87119    0  \n",
       "...          ...      ...  ...  \n",
       "7293505  49800.0  5.51203    0  \n",
       "7293506  49800.0  3.52480    0  \n",
       "7293507  49800.0  4.03249    0  \n",
       "7293508  49800.0  5.88917    0  \n",
       "7293509  49800.0  5.07687    0  \n",
       "\n",
       "[7293510 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1 = pd.read_csv('preprocess_train_data.csv').drop(columns=['제품']).fillna(0)\n",
    "train1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = train1.groupby('ID')\n",
    "\n",
    "scale_min_dict = {}\n",
    "scale_max_dict = {}\n",
    "\n",
    "for name, group in groups:\n",
    "    scale_min_dict[name] = group['판매량'].min()\n",
    "    scale_max_dict[name] = group['판매량'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "def scale_series(s):\n",
    "    return pd.Series(scaler.fit_transform(s.values.reshape(-1, 1)).flatten(), index=s.index)\n",
    "\n",
    "train1['판매량'] = train1.groupby('ID')['판매량'].transform(scale_series)\n",
    "train1['개당판매금액'] = scaler.fit_transform(train1['개당판매금액'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_columns = ['대분류', '중분류', '소분류', '브랜드']\n",
    "\n",
    "for col in categorical_columns:\n",
    "    label_encoder.fit(train1[col])\n",
    "    train1[col] = label_encoder.transform(train1[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler\n",
    "\n",
    "def weightedsampler(train_data):\n",
    "    train_category = train_data[:,0,1].astype(int)\n",
    "\n",
    "    cat_count = np.array([len(np.where(train_category==t)[0]) for t in np.unique(train_category)])\n",
    "    weight = 1. / cat_count\n",
    "    samples_weight = np.array([weight[t] for t in train_category])\n",
    "    samples_weight = torch.from_numpy(samples_weight)\n",
    "    sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return torch.Tensor(self.X[index]), torch.Tensor(self.Y[index])\n",
    "        return torch.Tensor(self.X[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class moving_avg(torch.nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = torch.nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "class series_decomp(torch.nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        residual = x - moving_mean\n",
    "        return moving_mean, residual \n",
    "        \n",
    "class LTSF_DLinear(torch.nn.Module):\n",
    "    def __init__(self, window_size, forcast_size, kernel_size, individual, feature_size):\n",
    "        super(LTSF_DLinear, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.forcast_size = forcast_size\n",
    "        self.decompsition = series_decomp(kernel_size)\n",
    "        self.individual = individual\n",
    "        self.channels = feature_size\n",
    "        if self.individual:\n",
    "            self.Linear_Seasonal = torch.nn.ModuleList()\n",
    "            self.Linear_Trend = torch.nn.ModuleList()\n",
    "            for i in range(self.channels):\n",
    "                self.Linear_Trend.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                # self.Linear_Trend[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "                self.Linear_Seasonal.append(torch.nn.Linear(self.window_size, self.forcast_size))\n",
    "                # self.Linear_Seasonal[i].weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "        else:\n",
    "            self.Linear_Trend = torch.nn.Linear(self.window_size, self.forcast_size)\n",
    "            # self.Linear_Trend.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "            self.Linear_Seasonal = torch.nn.Linear(self.window_size,  self.forcast_size)\n",
    "            # self.Linear_Seasonal.weight = torch.nn.Parameter((1/self.window_size)*torch.ones([self.forcast_size, self.window_size]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :, 1:] # 모델 학습 시 ID 제외\n",
    "        trend_init, seasonal_init = self.decompsition(x)\n",
    "        trend_init, seasonal_init = trend_init.permute(0,2,1), seasonal_init.permute(0,2,1)\n",
    "        if self.individual:\n",
    "            trend_output = torch.zeros([trend_init.size(0), trend_init.size(1), self.forcast_size], dtype=trend_init.dtype).to(trend_init.device)\n",
    "            seasonal_output = torch.zeros([seasonal_init.size(0), seasonal_init.size(1), self.forcast_size], dtype=seasonal_init.dtype).to(seasonal_init.device)\n",
    "            for idx in range(self.channels):\n",
    "                trend_output[:, idx, :] = self.Linear_Trend[idx](trend_init[:, idx, :])\n",
    "                seasonal_output[:, idx, :] = self.Linear_Seasonal[idx](seasonal_init[:, idx, :])                \n",
    "        else:\n",
    "            trend_output = self.Linear_Trend(trend_init)\n",
    "            seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        x = seasonal_output + trend_output\n",
    "        x = x.permute(0,2,1) # [batch_size, forcast_size, channels]\n",
    "        x = x[:, :, -1].squeeze(1) # 마지막 column(판매량)만 고려\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsymmetricMSELoss(nn.Module):\n",
    "    def __init__(self, alpha = 0.1):\n",
    "        super(AsymmetricMSELoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        diff = predictions - targets\n",
    "        squared_diff = diff ** 2\n",
    "        loss = torch.where(diff >= 0, self.alpha * squared_diff, (1 - self.alpha) * squared_diff)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class ASAM:\n",
    "    def __init__(self, optimizer, model, rho=0.5, eta=0.01):\n",
    "        self.optimizer = optimizer\n",
    "        self.model = model\n",
    "        self.rho = rho\n",
    "        self.eta = eta\n",
    "        self.state = defaultdict(dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def ascent_step(self):\n",
    "        wgrads = []\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            t_w = self.state[p].get(\"eps\")\n",
    "            if t_w is None:\n",
    "                t_w = torch.clone(p).detach()\n",
    "                self.state[p][\"eps\"] = t_w\n",
    "            if 'weight' in n:\n",
    "                t_w[...] = p[...]\n",
    "                t_w.abs_().add_(self.eta)\n",
    "                p.grad.mul_(t_w)\n",
    "            wgrads.append(torch.norm(p.grad, p=2))\n",
    "        wgrad_norm = torch.norm(torch.stack(wgrads), p=2) + 1.e-16\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            t_w = self.state[p].get(\"eps\")\n",
    "            if 'weight' in n:\n",
    "                p.grad.mul_(t_w)\n",
    "            eps = t_w\n",
    "            eps[...] = p.grad[...]\n",
    "            eps.mul_(self.rho / wgrad_norm)\n",
    "            p.add_(eps)\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def descent_step(self):\n",
    "        for n, p in self.model.named_parameters():\n",
    "            if p.grad is None:\n",
    "                continue\n",
    "            p.sub_(self.state[p][\"eps\"])\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "def compute_for_cat(cat_df, ids):\n",
    "    sub_cat_df = cat_df[cat_df[:, 0] == ids]\n",
    "    true_Y, pred_Y = sub_cat_df[:, 2], sub_cat_df[:, 3]\n",
    "\n",
    "    days_denom = np.sum(true_Y) + 1e-10\n",
    "    days_num = np.abs(true_Y - pred_Y)\n",
    "    days_denom_with_eps = np.maximum(true_Y, pred_Y) + 1e-10\n",
    "    days = (days_num / days_denom_with_eps) * (true_Y / days_denom)\n",
    "    return np.sum(days)\n",
    "\n",
    "def compute_PSFA(sub_df):\n",
    "    psfa_m = np.zeros(5)\n",
    "\n",
    "    for cat in range(5):\n",
    "        cat_df = sub_df[sub_df[:, :, 1] == cat]\n",
    "        cat_ids, _ = np.unique(cat_df[:, 0], return_counts=True)\n",
    "        cat_id_list = np.zeros(len(cat_ids))\n",
    "\n",
    "        results = Parallel(n_jobs=-1)(delayed(compute_for_cat)(cat_df, ids) for _, ids in enumerate(cat_ids))\n",
    "\n",
    "        cat_id_list = np.array(results)\n",
    "        psfa_m[cat] = 1 - (np.sum(cat_id_list) / len(cat_id_list))\n",
    "    return np.mean(psfa_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, minimizer, train_loader, val_loader, device, scheduler):\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1], output_device=0)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = AsymmetricMSELoss().to(device)\n",
    "    # criterion = nn.HuberLoss(delta=0.1).to(device)\n",
    "\n",
    "    best_score = 0\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for X, Y in tqdm(iter(train_loader)):\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            \n",
    "            # Ascent Step\n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            loss.backward()\n",
    "            minimizer.ascent_step()\n",
    "\n",
    "            # Descent Step\n",
    "            loss_2 = criterion(model(X), Y)\n",
    "            loss_2.backward()\n",
    "            minimizer.descent_step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss, psfa = validation(model, val_loader, criterion, device)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        model_save = ''\n",
    "        if psfa > best_score:\n",
    "            best_score = psfa\n",
    "            best_model = model\n",
    "            model_save = 'Model Saved'\n",
    "        \n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] PSFA : [{psfa:.5f}] {model_save}')\n",
    "    return best_model, best_score\n",
    "\n",
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    sub_df_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X, Y in tqdm(iter(val_loader)):\n",
    "            X = X.to(device)\n",
    "            Y = Y.to(device)\n",
    "            \n",
    "            output = model(X)\n",
    "            loss = criterion(output, Y)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            X_ids = X[:, :21, :2].detach().cpu().numpy()\n",
    "            Y = Y.detach().cpu().numpy().reshape(Y.shape[0], Y.shape[1], 1)\n",
    "            pred_Y = output.detach().cpu().numpy().reshape(output.shape[0], output.shape[1], 1)\n",
    "            \n",
    "            sub_df = np.concatenate([X_ids, Y, pred_Y], axis=2)\n",
    "\n",
    "            sub_df_list.append(sub_df)\n",
    "        \n",
    "        sub_df = np.concatenate(sub_df_list, axis=0)\n",
    "        PSFA = compute_PSFA(sub_df)\n",
    "    \n",
    "    return np.mean(val_loss) , PSFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X in tqdm(iter(test_loader)):\n",
    "            X = X.to(device)\n",
    "            output = model(X)\n",
    "            output = output.cpu().numpy()\n",
    "            predictions.extend(output)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_input = np.load('./data/new_data/train1_input_mean_stds.npy')\n",
    "train1_target = np.load('./data/new_data/train1_target_mean_stds.npy')\n",
    "test_input = np.load('./data/new_data/test_input_mean_stds.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = train1_input[:, 0, 0].astype(int)\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_input, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold_num, (train_idx, valid_idx) in enumerate(skf.split(train1_input, train_input_ids)):\n",
    "    print(f'fold: {fold_num}', '='*50)\n",
    "    \n",
    "    train_input, train_target = train1_input[train_idx], train1_target[train_idx]\n",
    "    val_input, val_target = train1_input[valid_idx], train1_target[valid_idx]\n",
    "\n",
    "    train_dataset = CustomDataset(train_input, train_target)\n",
    "    train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=16, pin_memory=True, sampler=weightedsampler(train_input))\n",
    "\n",
    "    val_dataset = CustomDataset(val_input, val_target)\n",
    "    val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    model = LTSF_DLinear(window_size=CFG['TRAIN_WINDOW_SIZE'], forcast_size=CFG[\"PREDICT_SIZE\"], kernel_size=25,individual=False, feature_size=1)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "    minimizer = ASAM(optimizer, model)\n",
    "\n",
    "    scheduler = None\n",
    "\n",
    "    print(\"Start Training\")\n",
    "    infer_model, fold_score = train(model, minimizer, train_loader, val_loader, device, scheduler)\n",
    "\n",
    "    pred = inference(infer_model, test_loader, device)\n",
    "\n",
    "    # 추론 결과를 inverse scaling\n",
    "    for idx in range(len(pred)):\n",
    "        pred[idx, :] = pred[idx, :] * (scale_max_dict[idx] - scale_min_dict[idx]) + scale_min_dict[idx]\n",
    "\n",
    "    torch.save(infer_model.state_dict(), f'./data/ensemble_submit/linear_idsplit_{fold_num}_{fold_score}.pth')\n",
    "\n",
    "    pred = np.round(pred, 0).astype(int)\n",
    "    \n",
    "    submit = pd.read_csv('./sample_submission.csv')\n",
    "    submit.iloc[:,1:] = pred\n",
    "    submit.to_csv(f'/home/hwlee/dacon/LGAI/ensemble_submit/linear_idsplit_{fold_num}_{fold_score}.csv', index=False)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "    del train_input, train_target, val_input, val_target, train_dataset, train_loader, val_dataset, val_loader, model, optimizer, scheduler, infer_model, pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lgai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
